{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ce82ef1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'plot_confusion_matrix' from 'sklearn.metrics' (C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mneighbors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KNeighborsClassifier \u001b[38;5;66;03m# K-Nearest Neighbors ML classifier (default n. of neighbors = 5)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#from scikitplot.metrics import plot_confusion_matrix # For plotting confusion matrices\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_confusion_matrix\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score \u001b[38;5;66;03m# For getting the accuracy of a model's predictions\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report \u001b[38;5;66;03m# Various metrics for model performance\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'plot_confusion_matrix' from 'sklearn.metrics' (C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler # For scaling to unit scale, before PCA application\n",
    "from sklearn.decomposition import PCA # For PCA dimensionality reduction technique\n",
    "import matplotlib.pyplot as plt # MatPlotLib for graphing data visually. Seaborn more likely to be used.\n",
    "from sklearn.preprocessing import LabelBinarizer # For converting categorical data into numeric, for modeling stage\n",
    "from sklearn.model_selection import StratifiedKFold # For optimal train_test splitting, for model input data\n",
    "from sklearn.model_selection import train_test_split # For basic dataset splitting\n",
    "from sklearn.neighbors import KNeighborsClassifier # K-Nearest Neighbors ML classifier (default n. of neighbors = 5)\n",
    "#from scikitplot.metrics import plot_confusion_matrix # For plotting confusion matrices\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import accuracy_score # For getting the accuracy of a model's predictions\n",
    "from sklearn.metrics import classification_report # Various metrics for model performance\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a961b20e",
   "metadata": {},
   "source": [
    "# Useful environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0cf488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Reduced dimensions' variable for altering the number of PCA principal components. Can be altered for needs.\n",
    "# Only 7 principal components needed when using non-normalised PCA dataset.\n",
    "dimensions_num_for_PCA = 7\n",
    "\n",
    "# Max number of permutations to run. Can be altered for needs.\n",
    "number_of_permutations = 100\n",
    "\n",
    "# 10 folds is usually the heuristic to follow for larger datasets of around this size.\n",
    "num_of_splits_for_skf = 10\n",
    "# Seed value to pass into models so that repeated runs result in the same output\n",
    "seed_val = 1\n",
    "\n",
    "# Number of statistical distance measures to run (for the results, columns section)\n",
    "num_of_statistical_dist_measures = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b805ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_PCA_feature_names(num_of_pca_components):\n",
    "    feature_names = []\n",
    "    for i in range(num_of_pca_components):    \n",
    "        feature_names.append(f\"Principal component {i+1}\")\n",
    "    return feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4b1468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See documentation above to understand what each step does, and why.\n",
    "def train_model_predict(model, model_name, X, y, skf):\n",
    "    for train_index, test_index in skf.split(X, y): # 1)\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index] # 2)\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        reshaped_y_train = np.asarray(y_train).reshape(-1, 1) # 3)\n",
    "        reshaped_y_test = np.asarray(y_test).reshape(-1, 1)\n",
    "        \n",
    "    model.fit(X_train, reshaped_y_train.ravel()) # 4)\n",
    "    pred_y = model.predict(X_test) # 5)\n",
    "    score = classification_report(reshaped_y_test, pred_y) # 6)\n",
    "    print('Classification report: \\n', score, '\\n')\n",
    "    cm=plot_confusion_matrix(reshaped_y_test, pred_y)\n",
    "        \n",
    "    return accuracy_score(reshaped_y_test, pred_y), X_train, X_test, y_train, pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391a8191",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3498f224",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2fa2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31e0d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf5c6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = data.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfef7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d27f8fd",
   "metadata": {},
   "source": [
    "# Fixing issues with ScikitLearn's PCA transform on this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec1cc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(df):\n",
    "    assert isinstance(df, pd.DataFrame), \"df needs to be a pd.DataFrame\"\n",
    "    df.dropna(inplace=True)\n",
    "    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(axis=1)\n",
    "    return df[indices_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757b5474",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned = data.copy()\n",
    "data_cleaned = clean_dataset(data_cleaned) # see methods at top of notebook\n",
    "data_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e94599",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned = data_cleaned.reset_index()\n",
    "# Removing un-needed index column added by reset_index method\n",
    "data_cleaned.drop('index', axis=1, inplace=True)\n",
    "data_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5380f658",
   "metadata": {},
   "source": [
    "### Considerations before PCA can be used correctly (before Data Preparation feature selection via PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d33bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88748797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the label attribute before dropping it.\n",
    "data_labels = data_cleaned['label']\n",
    "# Shows all the possible labels/ classes a model can predict.\n",
    "# Need to alter these to numeric 0, 1, etc... for model comprehension (e.g. pd.get_dummies()).\n",
    "data_labels.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a001d5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Axis=1 means columns. Axis=0 means rows. inplace=False means that the original 'df' isn't altered.\n",
    "data_no_labels = data_cleaned.drop('label', axis=1, inplace=False)\n",
    "# Getting feature names for the StandardScaler process\n",
    "data_features = data_no_labels.columns.tolist()\n",
    "# Printing out Dataframe with no label column, to show successful dropping\n",
    "data_no_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06757d8c",
   "metadata": {},
   "source": [
    "# Using StandardScaler to transform features into unit scale (optional for PCA)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3612a03b",
   "metadata": {},
   "source": [
    "StandardScaler() is an imported model from the sklearn.preprocessing library.\n",
    "It scales the specified Pandas Dataframe or Series object values to unit scale/ variance. \n",
    "This is usually required for certain functions to perform correctly, e.g. the PCA transform later (see docs: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
    "   \n",
    "StandardScaler().fit_transform(df) fits the StandardScaler model to the data, and transforms it into unit scale.\n",
    "pd.Dataframe(data=x, columns=y) can convert the data 'x' into a Pandas Dataframe object, using the respective columns 'y'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11af8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_no_labels.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2e2978",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.any(np.isnan(data_no_labels)))\n",
    "\n",
    "print(np.any(np.isfinite(data_no_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084a6e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "final=data_no_labels.replace([np.inf, -np.inf], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c31e478",
   "metadata": {},
   "outputs": [],
   "source": [
    "final.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a06f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.any(np.isnan(final)))\n",
    "print(np.all(np.isfinite(final)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effff5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scaled = StandardScaler().fit_transform(final.select_dtypes(include=['float64','int64']))\n",
    "# Converting back to dataframe\n",
    "data_scaled = pd.DataFrame(data = data_scaled, columns = data_features)\n",
    "data_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f456d8",
   "metadata": {},
   "source": [
    "# Plotting principle component variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d82e70",
   "metadata": {},
   "source": [
    "####  The plot below shows that using the first 30 PCA components actually describes most/ all (99.9%) of the variation (information) within the Normalised dataset. This is a huge dimension reduction from the initial 78 features, down to just 30."
   ]
  },
  {
   "cell_type": "raw",
   "id": "9a925306",
   "metadata": {},
   "source": [
    "Thus, looking at the Environment Variables (at the top of the notebook), the 'dimensions_num_for_PCA' variable will be set to between 10 and 25 based upon this evidence, to maximise efficiency and also dataset accuracy (after PCA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f0513c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_test = PCA().fit(data_scaled)\n",
    "plt.plot(np.cumsum(pca_test.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fd0b16ad",
   "metadata": {},
   "source": [
    "This Scree plot shows the variance explained by each principal component within the un-normalised dataset.\n",
    "Therefore, using the un-normalised dataset for PCA, only around 5 to 7 principal components are needed to explain the entire (99.9...%) variation of the original dataset. This is a very large dimension reduction.\n",
    "\n",
    "Thus, looking at the Environment Variables (at the top of the notebook), the 'dimensions_num_for_PCA' variable will be set to between 5 and 7 based upon this evidence, to maximise efficiency and also dataset accuracy (after PCA). If the non-normalised dataset is being used, of course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0d4f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The df_no_labels dataset holds the un-normalised dataset.\n",
    "pca_test = PCA().fit(data_no_labels)\n",
    "plt.plot(np.cumsum(pca_test.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d6496b",
   "metadata": {},
   "source": [
    "# Important note on these above scree plot results "
   ]
  },
  {
   "cell_type": "raw",
   "id": "4b10a1a3",
   "metadata": {},
   "source": [
    "The results of these graphs can change/ vary between different datasets used. Thus, these need to be inspected each time a new dataset (.csv file) is to be run on this code. Then, using the information the plots show, one should alter the environment variables, and thus the Neural Network/ MLP model's hyperparamters (tuning the Neural Network hyperparameters relies on knowing and using the number of features in a dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad20989f",
   "metadata": {},
   "source": [
    "# Now fitting and transforming the data with PCA"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bda14a89",
   "metadata": {},
   "source": [
    "Thus, the optimal number of principle components is set to the environment variable and this is now used to produce the appropriate multi-dimensional principle component array. This will be formatted back to a Pandas dataframe afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4d2592",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=dimensions_num_for_PCA)\n",
    "#principal_components = pca.fit(df_scaled).transform(df_scaled) => for normalised PCA\n",
    "\n",
    "# Non-normalised PCA\n",
    "principal_components = pca.fit(data_no_labels).transform(data_no_labels)\n",
    "principal_components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff91aa7d",
   "metadata": {},
   "source": [
    "# Getting Principal Component feature names, dynamically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911a5952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See Methods at the top of the notebook\n",
    "principal_component_headings = get_PCA_feature_names(dimensions_num_for_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9273dea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pc = pd.DataFrame(data = principal_components, columns = principal_component_headings)\n",
    "data_pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82275cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final = pd.concat([data_pc, data_labels], axis = 1)\n",
    "# Scroll to the RHS end of dataframe to see attached label feature\n",
    "data_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e168f5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelBinarizer()\n",
    "data_final['label'] = lb.fit_transform(data_final['label'])\n",
    "data_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac27cf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before LabelBinarizer: \", data_labels.unique())\n",
    "print(\"After LabelBinarizer: \", data_final['label'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b04f331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating the label so that the answers aren't provided to the model, in training.\n",
    "X = data_final.drop(['label'], axis = 1)\n",
    "y = data_final['label']\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6316e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=num_of_splits_for_skf, shuffle=False)\n",
    "skf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4a54db",
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    reshaped_y_train = np.asarray(y_train).reshape(-1, 1)\n",
    "    reshaped_y_test = np.asarray(y_test).reshape(-1, 1)\n",
    "    \n",
    "print( 'X_train length: ', len(X_train) ) # To check if splits worked\n",
    "print( 'y_train length: ', len(y_train) )\n",
    "print( 'X_test length: ', len(X_test) )\n",
    "print( 'y_test length: ', len(y_test) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74db73a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model = KNeighborsClassifier(n_neighbors=5, weights='uniform',\n",
    "                                    algorithm='auto', leaf_size=30,\n",
    "                                    p=2, metric='minkowski',\n",
    "                                    metric_params=None, n_jobs=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f49d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpacking the method return values. Last 4 are needed for statistical distance measure methods.\n",
    "accuracy, X_train, X_test, y_train, pred_y = train_model_predict(knn_model, \"K-Nearest Neighbor\", X, y, skf)\n",
    "print(\"Model accuracy= \", accuracy*100, \"%\\n\")\n",
    "print(\"Dataset labels: \", data_labels.unique())\n",
    "print(\"Dataset numeric labels after encoding for model: \", data_final['label'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ada0810",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
